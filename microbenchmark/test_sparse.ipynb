{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from pytorch_block_sparse import BlockSparseLinear\n",
    "\n",
    "import torch_sparse\n",
    "from torch_sparse import spmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_tensor(size, sparsity):\n",
    "    rows, cols = size\n",
    "    num_elements = rows * cols\n",
    "    num_non_zero = int(num_elements * (1 - sparsity))\n",
    "\n",
    "    # Random indices for non-zero elements\n",
    "    indices = np.random.choice(num_elements, num_non_zero, replace=False)\n",
    "    indices = np.unravel_index(indices, (rows, cols))\n",
    "    indices = torch.LongTensor(indices)\n",
    "\n",
    "    # Random values for these indices\n",
    "    values = torch.randn(num_non_zero)\n",
    "\n",
    "    # Create sparse tensor\n",
    "    return torch.sparse_coo_tensor(indices, values, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1 0.95\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "nn.Module.to only accepts floating point or complex dtypes, but got desired dtype=torch.int32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m sparse_matrix_mat2 \u001b[38;5;241m=\u001b[39m create_sparse_tensor(size\u001b[38;5;241m=\u001b[39m(dmodel,num_tokens),\n\u001b[1;32m     23\u001b[0m                                     sparsity\u001b[38;5;241m=\u001b[39msparsity)\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m     24\u001b[0m dense_matrix_mat2 \u001b[38;5;241m=\u001b[39m sparse_matrix_mat2\u001b[38;5;241m.\u001b[39mto_dense()\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m---> 26\u001b[0m blocksparse_fc \u001b[38;5;241m=\u001b[39m \u001b[43mBlockSparseLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdff_shared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msparsity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tmp \u001b[38;5;241m=\u001b[39m sparse_matrix_mat1\u001b[38;5;241m.\u001b[39mcoalesce()\n\u001b[1;32m     29\u001b[0m index \u001b[38;5;241m=\u001b[39m tmp\u001b[38;5;241m.\u001b[39mindices()\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/taste/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (dtype\u001b[38;5;241m.\u001b[39mis_floating_point \u001b[38;5;129;01mor\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_complex):\n\u001b[0;32m-> 1145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn.Module.to only accepts floating point or complex \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1146\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtypes, but got desired dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[1;32m   1148\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex modules are a new feature under active development whose design may change, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand some modules might not work as expected when using complex tensors as parameters or buffers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif a complex module does not work as expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: nn.Module.to only accepts floating point or complex dtypes, but got desired dtype=torch.int32"
     ]
    }
   ],
   "source": [
    "warmup_iterations = 10\n",
    "total_iterations  = 100\n",
    "num_tokens_list   = [1, 64, 512]\n",
    "dmodel_list       = [1024, 4096, 4096*2]\n",
    "sparsity_list     = [0.95, 0.99, 0.999]\n",
    "\n",
    "result_dict = []\n",
    "\n",
    "for dmodel in dmodel_list:\n",
    "    dff_shared = dmodel * 3\n",
    "    for num_tokens in num_tokens_list:\n",
    "        for sparsity in sparsity_list:\n",
    "\n",
    "            print(dmodel, num_tokens, sparsity)\n",
    "\n",
    "            # Create a dense matrix (Weight)\n",
    "            sparse_matrix_mat1 = create_sparse_tensor(size=(dff_shared,dmodel),\n",
    "                                                        sparsity=sparsity).cuda().to(torch.int)\n",
    "            dense_matrix_mat1 = sparse_matrix_mat1.to_dense().cuda().to(torch.int)\n",
    "\n",
    "            # Create a sparse matrix (Activation)\n",
    "            sparse_matrix_mat2 = create_sparse_tensor(size=(dmodel,num_tokens),\n",
    "                                                sparsity=sparsity).cuda().to(torch.int)\n",
    "            dense_matrix_mat2 = sparse_matrix_mat2.to_dense().cuda().to(torch.int)\n",
    "\n",
    "            blocksparse_fc = BlockSparseLinear(dmodel, dff_shared, density=(1-sparsity)).to(torch.int)\n",
    "\n",
    "            tmp = sparse_matrix_mat1.coalesce()\n",
    "            index = tmp.indices()\n",
    "            value = tmp.values()\n",
    "\n",
    "            for i in range(warmup_iterations):\n",
    "                result = torch.sparse.mm(dense_matrix_mat1, sparse_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            t1 = time.time_ns()\n",
    "            for i in range(total_iterations):\n",
    "                result = torch.sparse.mm(dense_matrix_mat1, sparse_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "            t2 = time.time_ns()\n",
    "\n",
    "\n",
    "            for i in range(warmup_iterations):\n",
    "                result = torch.mm(dense_matrix_mat1, dense_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            t3 = time.time_ns()\n",
    "            for i in range(total_iterations):\n",
    "                result = torch.mm(dense_matrix_mat1, dense_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "            t4 = time.time_ns()\n",
    "\n",
    "\n",
    "            for i in range(warmup_iterations):\n",
    "                result = torch.sparse.mm(sparse_matrix_mat1, sparse_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            t5 = time.time_ns()\n",
    "            for i in range(total_iterations):\n",
    "                result = torch.sparse.mm(sparse_matrix_mat1, sparse_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "            t6 = time.time_ns()\n",
    "\n",
    "\n",
    "            for i in range(warmup_iterations):\n",
    "                result = blocksparse_fc(dense_matrix_mat1)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            t7 = time.time_ns()\n",
    "            for i in range(total_iterations):\n",
    "                result = blocksparse_fc(dense_matrix_mat1)\n",
    "            torch.cuda.synchronize()\n",
    "            t8 = time.time_ns()\n",
    "\n",
    "\n",
    "            for i in range(warmup_iterations):\n",
    "                result = torch_sparse.spmm(index, value, dff_shared, dmodel, dense_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            t9 = time.time_ns()\n",
    "            for i in range(total_iterations):\n",
    "                result = torch_sparse.spmm(index, value, dff_shared, dmodel, dense_matrix_mat2)\n",
    "            torch.cuda.synchronize()\n",
    "            t10 = time.time_ns()\n",
    "\n",
    "            sparse_gemm = (t2-t1) / 1.0e6\n",
    "            dense_gemm = (t4-t3) / 1.0e6\n",
    "            spsp_gemm = (t6-t5) / 1.0e6\n",
    "            bsp_gemm = (t8-t7) / 1.0e6\n",
    "            tsp_gemm = (t10-t9) / 1.0e6\n",
    "\n",
    "            result_dict.append({\n",
    "                \"dmodel\" : dmodel,\n",
    "                \"dff_shared\" : dff_shared,\n",
    "                \"tokens\" : num_tokens,\n",
    "                \"sparsity\" : sparsity,\n",
    "                \"dense_gemm\" : dense_gemm,\n",
    "                \"sparse_gemm\" : sparse_gemm,\n",
    "                \"spsp_gemm\" : spsp_gemm,\n",
    "                \"bsp_gemm\" : bsp_gemm,\n",
    "                \"tsp_gemm\" : tsp_gemm,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dmodel</th>\n",
       "      <th>dff_shared</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>dense_gemm</th>\n",
       "      <th>sparse_gemm</th>\n",
       "      <th>spsp_gemm</th>\n",
       "      <th>bsp_gemm</th>\n",
       "      <th>tsp_gemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1.247041</td>\n",
       "      <td>31.783795</td>\n",
       "      <td>89.500512</td>\n",
       "      <td>17.792116</td>\n",
       "      <td>7.009113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1.223108</td>\n",
       "      <td>31.683953</td>\n",
       "      <td>115.858394</td>\n",
       "      <td>10.875904</td>\n",
       "      <td>7.034448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.226122</td>\n",
       "      <td>17.448482</td>\n",
       "      <td>75.207081</td>\n",
       "      <td>9.259757</td>\n",
       "      <td>6.958705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>64</td>\n",
       "      <td>0.950</td>\n",
       "      <td>4.618658</td>\n",
       "      <td>34.412385</td>\n",
       "      <td>83.896912</td>\n",
       "      <td>17.701424</td>\n",
       "      <td>24.012630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990</td>\n",
       "      <td>3.845554</td>\n",
       "      <td>21.337367</td>\n",
       "      <td>57.465929</td>\n",
       "      <td>10.941535</td>\n",
       "      <td>4.754367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999</td>\n",
       "      <td>3.799233</td>\n",
       "      <td>20.596239</td>\n",
       "      <td>56.819286</td>\n",
       "      <td>9.024782</td>\n",
       "      <td>4.433968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>512</td>\n",
       "      <td>0.950</td>\n",
       "      <td>20.497444</td>\n",
       "      <td>56.634255</td>\n",
       "      <td>178.708913</td>\n",
       "      <td>17.788524</td>\n",
       "      <td>175.471630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>512</td>\n",
       "      <td>0.990</td>\n",
       "      <td>20.563533</td>\n",
       "      <td>32.886396</td>\n",
       "      <td>65.741869</td>\n",
       "      <td>10.932280</td>\n",
       "      <td>37.156623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1024</td>\n",
       "      <td>3072</td>\n",
       "      <td>512</td>\n",
       "      <td>0.999</td>\n",
       "      <td>20.624803</td>\n",
       "      <td>23.651566</td>\n",
       "      <td>52.329464</td>\n",
       "      <td>9.025160</td>\n",
       "      <td>4.458977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>13.510477</td>\n",
       "      <td>24.683024</td>\n",
       "      <td>274.088391</td>\n",
       "      <td>700.619937</td>\n",
       "      <td>27.739495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990</td>\n",
       "      <td>13.502192</td>\n",
       "      <td>20.475110</td>\n",
       "      <td>151.071329</td>\n",
       "      <td>274.794100</td>\n",
       "      <td>4.840132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>13.493821</td>\n",
       "      <td>20.159917</td>\n",
       "      <td>273.685829</td>\n",
       "      <td>138.258079</td>\n",
       "      <td>4.321831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>64</td>\n",
       "      <td>0.950</td>\n",
       "      <td>50.712565</td>\n",
       "      <td>150.258377</td>\n",
       "      <td>327.579345</td>\n",
       "      <td>788.500472</td>\n",
       "      <td>508.955292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990</td>\n",
       "      <td>72.235147</td>\n",
       "      <td>58.511800</td>\n",
       "      <td>112.832679</td>\n",
       "      <td>366.937521</td>\n",
       "      <td>97.466785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999</td>\n",
       "      <td>49.637855</td>\n",
       "      <td>29.898457</td>\n",
       "      <td>76.999947</td>\n",
       "      <td>264.646816</td>\n",
       "      <td>21.591575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>512</td>\n",
       "      <td>0.950</td>\n",
       "      <td>594.151015</td>\n",
       "      <td>1949.260110</td>\n",
       "      <td>1096.124873</td>\n",
       "      <td>1493.817565</td>\n",
       "      <td>5477.445701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>512</td>\n",
       "      <td>0.990</td>\n",
       "      <td>561.837514</td>\n",
       "      <td>450.416671</td>\n",
       "      <td>158.714221</td>\n",
       "      <td>808.061057</td>\n",
       "      <td>1309.435385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4096</td>\n",
       "      <td>12288</td>\n",
       "      <td>512</td>\n",
       "      <td>0.999</td>\n",
       "      <td>574.487114</td>\n",
       "      <td>103.392468</td>\n",
       "      <td>80.651112</td>\n",
       "      <td>293.205034</td>\n",
       "      <td>142.722882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>52.654056</td>\n",
       "      <td>46.331338</td>\n",
       "      <td>1710.802407</td>\n",
       "      <td>16245.617721</td>\n",
       "      <td>164.921341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990</td>\n",
       "      <td>51.933329</td>\n",
       "      <td>26.553219</td>\n",
       "      <td>355.565218</td>\n",
       "      <td>4578.301481</td>\n",
       "      <td>28.356036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>51.132734</td>\n",
       "      <td>20.382168</td>\n",
       "      <td>512.265324</td>\n",
       "      <td>1395.759019</td>\n",
       "      <td>4.938212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>64</td>\n",
       "      <td>0.950</td>\n",
       "      <td>434.659588</td>\n",
       "      <td>930.010087</td>\n",
       "      <td>1896.120324</td>\n",
       "      <td>17280.355023</td>\n",
       "      <td>3932.102942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990</td>\n",
       "      <td>498.635214</td>\n",
       "      <td>390.071813</td>\n",
       "      <td>401.314557</td>\n",
       "      <td>5252.959555</td>\n",
       "      <td>694.242607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999</td>\n",
       "      <td>365.119854</td>\n",
       "      <td>68.496567</td>\n",
       "      <td>98.422559</td>\n",
       "      <td>1386.830883</td>\n",
       "      <td>66.460809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>512</td>\n",
       "      <td>0.950</td>\n",
       "      <td>2941.195712</td>\n",
       "      <td>6789.478937</td>\n",
       "      <td>5688.860204</td>\n",
       "      <td>20150.924351</td>\n",
       "      <td>37910.456866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>512</td>\n",
       "      <td>0.990</td>\n",
       "      <td>2046.291623</td>\n",
       "      <td>2118.555787</td>\n",
       "      <td>570.263138</td>\n",
       "      <td>4489.217313</td>\n",
       "      <td>5261.255437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8192</td>\n",
       "      <td>24576</td>\n",
       "      <td>512</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1861.823168</td>\n",
       "      <td>414.597032</td>\n",
       "      <td>89.832588</td>\n",
       "      <td>1566.723891</td>\n",
       "      <td>508.255699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dmodel  dff_shared  tokens  sparsity   dense_gemm  sparse_gemm  \\\n",
       "0     1024        3072       1     0.950     1.247041    31.783795   \n",
       "1     1024        3072       1     0.990     1.223108    31.683953   \n",
       "2     1024        3072       1     0.999     1.226122    17.448482   \n",
       "3     1024        3072      64     0.950     4.618658    34.412385   \n",
       "4     1024        3072      64     0.990     3.845554    21.337367   \n",
       "5     1024        3072      64     0.999     3.799233    20.596239   \n",
       "6     1024        3072     512     0.950    20.497444    56.634255   \n",
       "7     1024        3072     512     0.990    20.563533    32.886396   \n",
       "8     1024        3072     512     0.999    20.624803    23.651566   \n",
       "9     4096       12288       1     0.950    13.510477    24.683024   \n",
       "10    4096       12288       1     0.990    13.502192    20.475110   \n",
       "11    4096       12288       1     0.999    13.493821    20.159917   \n",
       "12    4096       12288      64     0.950    50.712565   150.258377   \n",
       "13    4096       12288      64     0.990    72.235147    58.511800   \n",
       "14    4096       12288      64     0.999    49.637855    29.898457   \n",
       "15    4096       12288     512     0.950   594.151015  1949.260110   \n",
       "16    4096       12288     512     0.990   561.837514   450.416671   \n",
       "17    4096       12288     512     0.999   574.487114   103.392468   \n",
       "18    8192       24576       1     0.950    52.654056    46.331338   \n",
       "19    8192       24576       1     0.990    51.933329    26.553219   \n",
       "20    8192       24576       1     0.999    51.132734    20.382168   \n",
       "21    8192       24576      64     0.950   434.659588   930.010087   \n",
       "22    8192       24576      64     0.990   498.635214   390.071813   \n",
       "23    8192       24576      64     0.999   365.119854    68.496567   \n",
       "24    8192       24576     512     0.950  2941.195712  6789.478937   \n",
       "25    8192       24576     512     0.990  2046.291623  2118.555787   \n",
       "26    8192       24576     512     0.999  1861.823168   414.597032   \n",
       "\n",
       "      spsp_gemm      bsp_gemm      tsp_gemm  \n",
       "0     89.500512     17.792116      7.009113  \n",
       "1    115.858394     10.875904      7.034448  \n",
       "2     75.207081      9.259757      6.958705  \n",
       "3     83.896912     17.701424     24.012630  \n",
       "4     57.465929     10.941535      4.754367  \n",
       "5     56.819286      9.024782      4.433968  \n",
       "6    178.708913     17.788524    175.471630  \n",
       "7     65.741869     10.932280     37.156623  \n",
       "8     52.329464      9.025160      4.458977  \n",
       "9    274.088391    700.619937     27.739495  \n",
       "10   151.071329    274.794100      4.840132  \n",
       "11   273.685829    138.258079      4.321831  \n",
       "12   327.579345    788.500472    508.955292  \n",
       "13   112.832679    366.937521     97.466785  \n",
       "14    76.999947    264.646816     21.591575  \n",
       "15  1096.124873   1493.817565   5477.445701  \n",
       "16   158.714221    808.061057   1309.435385  \n",
       "17    80.651112    293.205034    142.722882  \n",
       "18  1710.802407  16245.617721    164.921341  \n",
       "19   355.565218   4578.301481     28.356036  \n",
       "20   512.265324   1395.759019      4.938212  \n",
       "21  1896.120324  17280.355023   3932.102942  \n",
       "22   401.314557   5252.959555    694.242607  \n",
       "23    98.422559   1386.830883     66.460809  \n",
       "24  5688.860204  20150.924351  37910.456866  \n",
       "25   570.263138   4489.217313   5261.255437  \n",
       "26    89.832588   1566.723891    508.255699  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result_dict)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoEBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
